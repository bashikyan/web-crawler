# Web Crawler

ÕŠÕ¡Ö€Õ¦ Ö‡ Õ¡Ö€Õ¤ÕµÕ¸Ö‚Õ¶Õ¡Õ¾Õ¥Õ¿ web crawler Python-Õ¸Õ¾ Õ«Ö€Õ¡Õ¯Õ¡Õ¶Õ¡ÖÕ¾Õ¡Õ®:

## Õ€Õ¶Õ¡Ö€Õ¡Õ¾Õ¸Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€

- ğŸ” Web Õ§Õ»Õ¥Ö€Õ« crawling/ÖÕ«Ö€Õ¸Ö‚ÕµÖ
- ğŸ“Š ÕÕ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ« Õ°Õ¡Õ¾Õ¡Ö„Õ¡Õ£Ö€Õ¸Ö‚Õ´ Ö‡ ÕºÕ¡Õ°ÕºÕ¡Õ¶Õ¸Ö‚Õ´
- ğŸš¦ Rate limiting Ö‡ respect robots.txt
- ğŸ“ JSON Ö‡ CSV Ö†Õ¸Ö€Õ´Õ¡Õ¿Õ¶Õ¥Ö€Õ¸Õ¾ Õ¡Ö€Õ¿Õ¡Õ°Õ¡Õ¶Õ¸Ö‚Õ´
- ğŸ“ˆ ÕÕ«Õ³Õ¡Õ¯Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ Ö‡ logging
- ğŸŒ Same-domain Ö‡ cross-domain crawling
- âš™ï¸ Ô¿Õ¡Ö€Õ£Õ¡Õ¾Õ¸Ö€Õ¾Õ¸Õ² ÕºÕ¡Ö€Õ¡Õ´Õ¥Õ¿Ö€Õ¥Ö€

## Ô¿Õ¡Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€Õ¨

Ô±Õ¶Õ°Ö€Õ¡ÕªÕ¥Õ·Õ¿ Python packages:

```bash
pip install requests beautifulsoup4 lxml
```

## ÕÕ¥Õ²Õ¡Õ¤Ö€Õ¸Ö‚Õ´

1. Clone repository:
```bash
git clone https://github.com/yourusername/web-crawler.git
cd web-crawler
```

2. ÕÕ¥Õ²Õ¡Õ¤Ö€Õ¥Ö„ Õ¯Õ¡Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€Õ¨:
```bash
pip install -r requirements.txt
```

## Õ•Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¸Ö‚Õ´

### Command Line Interface

Õ€Õ«Õ´Õ¶Õ¡Õ¯Õ¡Õ¶ Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¸Ö‚Õ´:
```bash
python crawler.py https://example.com
```

Ô¸Õ¶Õ¤Õ¬Õ¡ÕµÕ¶Õ¾Õ¡Õ® Ö…ÕºÖÕ«Õ¡Õ¶Õ¥Ö€:
```bash
python crawler.py https://example.com \
  --max-pages 100 \
  --delay 2 \
  --timeout 15 \
  --output my_crawl \
  --format json \
  --cross-domain
```

### Python API

```python
from crawler import WebCrawler

# ÕÕ¿Õ¥Õ²Õ®Õ¥Õ¬ crawler instance
crawler = WebCrawler(max_pages=50, delay=1)

# ÕÕ¯Õ½Õ¥Õ¬ crawling
crawler.crawl('https://example.com')

# ÕŠÕ¡Õ°ÕºÕ¡Õ¶Õ¥Õ¬ Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ¨
crawler.save_to_json('results.json')
crawler.save_to_csv('results.csv')

# ÕÕ¿Õ¡Õ¶Õ¡Õ¬ Õ¾Õ«Õ³Õ¡Õ¯Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶
stats = crawler.get_statistics()
print(stats)
```

## ÕŠÕ¡Ö€Õ¡Õ´Õ¥Õ¿Ö€Õ¥Ö€

| ÕŠÕ¡Ö€Õ¡Õ´Õ¥Õ¿Ö€ | Õ†Õ¯Õ¡Ö€Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ | Default Õ¡Ö€ÕªÕ¥Ö„ |
|-----------|----------------|---------------|
| `max_pages` | Ô±Õ¼Õ¡Õ¾Õ¥Õ¬Õ¡Õ£Õ¸Ö‚ÕµÕ¶ crawl Õ¡Ö€Õ¾Õ¸Õ² Õ§Õ»Õ¥Ö€Õ« Ö„Õ¡Õ¶Õ¡Õ¯ | 50 |
| `delay` | Delay requests-Õ¶Õ¥Ö€Õ« Õ´Õ«Õ»Ö‡ (Õ¾Ö€Õ¯) | 1 |
| `timeout` | Request timeout (Õ¾Ö€Õ¯) | 10 |
| `same_domain_only` | Crawl Õ´Õ«Õ¡ÕµÕ¶ Õ¶Õ¸Ö‚ÕµÕ¶ domain-Õ«Ö | True |

## Õ–Õ¡ÕµÕ¬Õ¡ÕµÕ«Õ¶ Õ¯Õ¡Õ¼Õ¸Ö‚ÖÕ¾Õ¡Õ®Ö„

```
web-crawler/
â”œâ”€â”€ crawler.py          # Õ€Õ«Õ´Õ¶Õ¡Õ¯Õ¡Õ¶ crawler implementation
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md          # Õ†Õ¡Õ­Õ¡Õ£Õ®Õ« Õ¶Õ¯Õ¡Ö€Õ¡Õ£Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶
â”œâ”€â”€ .gitignore         # Git ignore rules
â”œâ”€â”€ examples/          # Õ•Ö€Õ«Õ¶Õ¡Õ¯Õ¶Õ¥Ö€
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â””â”€â”€ advanced_usage.py
â””â”€â”€ tests/             # Unit tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_crawler.py
    â””â”€â”€ test_utils.py
```

## Ô±Ö€Õ¿Õ¡Õ°Õ¡Õ¶Õ´Õ¡Õ¶ Ö†Õ¸Ö€Õ´Õ¡Õ¿Õ¶Õ¥Ö€

### JSON Output
```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "description": "Example website",
  "headings": [
    {"tag": "h1", "text": "Example Domain"}
  ],
  "text_preview": "This domain is for use in illustrative examples...",
  "images": [
    {"url": "https://example.com/image.jpg", "alt": "Example image"}
  ],
  "crawl_timestamp": "2024-01-15T10:30:45",
  "links_count": 15
}
```

### CSV Output
CSV Ö†Õ¡ÕµÕ¬Õ¨ ÕºÕ¡Ö€Õ¸Ö‚Õ¶Õ¡Õ¯Õ¸Ö‚Õ´ Õ§ Õ°Õ¥Õ¿Ö‡ÕµÕ¡Õ¬ columns:
- url
- title
- description
- text_preview
- links_count
- crawl_timestamp

## Õ•Ö€Õ«Õ¶Õ¡Õ¯Õ¶Õ¥Ö€

### Õ€Õ«Õ´Õ¶Õ¡Õ¯Õ¡Õ¶ Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¸Ö‚Õ´
```python
crawler = WebCrawler(max_pages=10)
crawler.crawl('https://news.am')
crawler.save_to_json('news_crawl.json')
```

### Ô²Õ¡Õ¦Õ´Õ¡Õ©Õ«Õ¾ domains
```python
crawler = WebCrawler(max_pages=50, delay=2)
crawler.crawl('https://example.com', same_domain_only=False)
```

### Ô¿Õ¡Ö€Õ£Õ¡Õ¾Õ¸Ö€Õ¾Õ¡Õ® headers
```python
crawler = WebCrawler()
crawler.headers['Accept-Language'] = 'hy,en;q=0.9'
crawler.crawl('https://example.com')
```

## Logging

Crawler-Õ¨ Õ¡ÕºÕ¡Õ°Õ¸Õ¾Õ¸Ö‚Õ´ Õ§ Õ´Õ¡Õ¶Ö€Õ¡Õ´Õ¡Õ½Õ¶ logging:
- Console output real-time progress-Õ« Õ°Õ¡Õ´Õ¡Ö€
- `crawler.log` Ö†Õ¡ÕµÕ¬ Õ´Õ¡Õ¶Ö€Õ¡Õ´Õ¡Õ½Õ¶ logs-Õ« Õ°Õ¡Õ´Õ¡Ö€

## ÕÕ¡Õ°Õ´Õ¡Õ¶Õ¡ÖƒÕ¡Õ¯Õ¸Ö‚Õ´Õ¶Õ¥Ö€

- Õ€Õ¡Ö€Õ£Õ¸Ö‚Õ´ Õ§ robots.txt (Õ¡ÕºÕ¡Õ£Õ¡ Õ¡Õ´Õ½Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶)
- Õ‰Õ« Õ¡Õ»Õ¡Õ¯ÖÕ¸Ö‚Õ´ JavaScript-rendered content
- Õ‰Õ« Õ¡Õ»Õ¡Õ¯ÖÕ¸Ö‚Õ´ session management
- Ô¿Õ¡Õ­Õ¾Õ¡Õ® Õ§ content-type headers-Õ«Ö

## Ô¶Õ¡Ö€Õ£Õ¡ÖÕ¸Ö‚Õ´

### Tests Õ£Õ¸Ö€Õ®Õ¡Ö€Õ¯Õ¸Ö‚Õ´
```bash
python -m pytest tests/
```

### Code formatting
```bash
black crawler.py
```

## Ô¼Õ«ÖÕ¥Õ¶Õ¦Õ«Õ¡

MIT License - Õ¿Õ¥Õ½ LICENSE Ö†Õ¡ÕµÕ¬Õ¨ Õ´Õ¡Õ¶Ö€Õ¡Õ´Õ¡Õ½Õ¶Õ¥Ö€Õ« Õ°Õ¡Õ´Õ¡Ö€:

## Õ†Õ¥Ö€Õ¤Ö€Õ¸Ö‚Õ´

Pull requests-Õ¨ Õ¸Õ²Õ»Õ¸Ö‚Õ¶Õ¾Õ¸Ö‚Õ´ Õ¥Õ¶! Ô½Õ¶Õ¤Õ«Ö€Õ¶Õ¥Ö€Õ« Õ¯Õ¡Õ´ Õ¢Õ¡Ö€Õ¥Õ¬Õ¡Õ¾Õ¸Ö‚Õ´Õ¶Õ¥Ö€Õ« Õ°Õ¡Õ´Õ¡Ö€:

1. Fork repository
2. ÕÕ¿Õ¥Õ²Õ®Õ¥Ö„ feature branch (`git checkout -b feature/amazing-feature`)
3. Commit ÖƒÕ¸ÖƒÕ¸Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€Õ¨ (`git commit -m 'Add amazing feature'`)
4. Push branch-Õ¨ (`git push origin feature/amazing-feature`)
5. Ô²Õ¡ÖÕ¥Ö„ Pull Request


