# Web Crawler

’ä’°÷Ä’¶ ÷á ’°÷Ä’§’µ’∏÷Ç’∂’°’æ’•’ø web crawler Python-’∏’æ ’´÷Ä’°’Ø’°’∂’°÷Å’æ’°’Æ:

## ’Ä’∂’°÷Ä’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä

- üîç Web ’ß’ª’•÷Ä’´ crawling/÷Å’´÷Ä’∏÷Ç’µ÷Å
- üìä ’è’æ’µ’°’¨’∂’•÷Ä’´ ’∞’°’æ’°÷Ñ’°’£÷Ä’∏÷Ç’¥ ÷á ’∫’°’∞’∫’°’∂’∏÷Ç’¥
- üö¶ Rate limiting ÷á respect robots.txt
- üìÅ JSON ÷á CSV ÷Ü’∏÷Ä’¥’°’ø’∂’•÷Ä’∏’æ ’°÷Ä’ø’°’∞’°’∂’∏÷Ç’¥
- üìà ’é’´’≥’°’Ø’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ÷á logging
- üåê Same-domain ÷á cross-domain crawling
- ‚öôÔ∏è ‘ø’°÷Ä’£’°’æ’∏÷Ä’æ’∏’≤ ’∫’°÷Ä’°’¥’•’ø÷Ä’•÷Ä

## ‘ø’°’≠’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®

‘±’∂’∞÷Ä’°’™’•’∑’ø Python packages:

```bash
pip install requests beautifulsoup4 lxml
```

## ’è’•’≤’°’§÷Ä’∏÷Ç’¥

1. Clone repository:
```bash
git clone https://github.com/yourusername/web-crawler.git
cd web-crawler
```

2. ’è’•’≤’°’§÷Ä’•÷Ñ ’Ø’°’≠’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®:
```bash
pip install -r requirements.txt
```

## ’ï’£’ø’°’£’∏÷Ä’Æ’∏÷Ç’¥

### Command Line Interface

’Ä’´’¥’∂’°’Ø’°’∂ ÷Ö’£’ø’°’£’∏÷Ä’Æ’∏÷Ç’¥:
```bash
python crawler.py https://example.com
```

‘∏’∂’§’¨’°’µ’∂’æ’°’Æ ÷Ö’∫÷Å’´’°’∂’•÷Ä:
```bash
python crawler.py https://example.com \
  --max-pages 100 \
  --delay 2 \
  --timeout 15 \
  --output my_crawl \
  --format json \
  --cross-domain
```

### Python API

```python
from crawler import WebCrawler

# ’ç’ø’•’≤’Æ’•’¨ crawler instance
crawler = WebCrawler(max_pages=50, delay=1)

# ’ç’Ø’Ω’•’¨ crawling
crawler.crawl('https://example.com')

# ’ä’°’∞’∫’°’∂’•’¨ ’ø’æ’µ’°’¨’∂’•÷Ä’®
crawler.save_to_json('results.json')
crawler.save_to_csv('results.csv')

# ’ç’ø’°’∂’°’¨ ’æ’´’≥’°’Ø’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂
stats = crawler.get_statistics()
print(stats)
```

## ’ä’°÷Ä’°’¥’•’ø÷Ä’•÷Ä

| ’ä’°÷Ä’°’¥’•’ø÷Ä | ’Ü’Ø’°÷Ä’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ | Default ’°÷Ä’™’•÷Ñ |
|-----------|----------------|---------------|
| `max_pages` | ‘±’º’°’æ’•’¨’°’£’∏÷Ç’µ’∂ crawl ’°÷Ä’æ’∏’≤ ’ß’ª’•÷Ä’´ ÷Ñ’°’∂’°’Ø | 50 |
| `delay` | Delay requests-’∂’•÷Ä’´ ’¥’´’ª÷á (’æ÷Ä’Ø) | 1 |
| `timeout` | Request timeout (’æ÷Ä’Ø) | 10 |
| `same_domain_only` | Crawl ’¥’´’°’µ’∂ ’∂’∏÷Ç’µ’∂ domain-’´÷Å | True |

## ’ñ’°’µ’¨’°’µ’´’∂ ’Ø’°’º’∏÷Ç÷Å’æ’°’Æ÷Ñ

```
web-crawler/
‚îú‚îÄ‚îÄ crawler.py          # ’Ä’´’¥’∂’°’Ø’°’∂ crawler implementation
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ README.md          # ’Ü’°’≠’°’£’Æ’´ ’∂’Ø’°÷Ä’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂
‚îú‚îÄ‚îÄ .gitignore         # Git ignore rules
‚îú‚îÄ‚îÄ examples/          # ’ï÷Ä’´’∂’°’Ø’∂’•÷Ä
‚îÇ   ‚îú‚îÄ‚îÄ basic_usage.py
‚îÇ   ‚îî‚îÄ‚îÄ advanced_usage.py
‚îî‚îÄ‚îÄ tests/             # Unit tests
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_crawler.py
    ‚îî‚îÄ‚îÄ test_utils.py
```

## ‘±÷Ä’ø’°’∞’°’∂’¥’°’∂ ÷Ü’∏÷Ä’¥’°’ø’∂’•÷Ä

### JSON Output
```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "description": "Example website",
  "headings": [
    {"tag": "h1", "text": "Example Domain"}
  ],
  "text_preview": "This domain is for use in illustrative examples...",
  "images": [
    {"url": "https://example.com/image.jpg", "alt": "Example image"}
  ],
  "crawl_timestamp": "2024-01-15T10:30:45",
  "links_count": 15
}
```

### CSV Output
CSV ÷Ü’°’µ’¨’® ’∫’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’ß ’∞’•’ø÷á’µ’°’¨ columns:
- url
- title
- description
- text_preview
- links_count
- crawl_timestamp

## ’ï÷Ä’´’∂’°’Ø’∂’•÷Ä

### ’Ä’´’¥’∂’°’Ø’°’∂ ÷Ö’£’ø’°’£’∏÷Ä’Æ’∏÷Ç’¥
```python
crawler = WebCrawler(max_pages=10)
crawler.crawl('https://news.am')
crawler.save_to_json('news_crawl.json')
```

### ‘≤’°’¶’¥’°’©’´’æ domains
```python
crawler = WebCrawler(max_pages=50, delay=2)
crawler.crawl('https://example.com', same_domain_only=False)
```

### ‘ø’°÷Ä’£’°’æ’∏÷Ä’æ’°’Æ headers
```python
crawler = WebCrawler()
crawler.headers['Accept-Language'] = 'hy,en;q=0.9'
crawler.crawl('https://example.com')
```

## Logging

Crawler-’® ’°’∫’°’∞’∏’æ’∏÷Ç’¥ ’ß ’¥’°’∂÷Ä’°’¥’°’Ω’∂ logging:
- Console output real-time progress-’´ ’∞’°’¥’°÷Ä
- `crawler.log` ÷Ü’°’µ’¨ ’¥’°’∂÷Ä’°’¥’°’Ω’∂ logs-’´ ’∞’°’¥’°÷Ä

## ’ç’°’∞’¥’°’∂’°÷É’°’Ø’∏÷Ç’¥’∂’•÷Ä

- ’Ä’°÷Ä’£’∏÷Ç’¥ ’ß robots.txt (’°’∫’°’£’° ’°’¥’Ω’∏÷Ç’©’µ’∏÷Ç’∂)
- ’â’´ ’°’ª’°’Ø÷Å’∏÷Ç’¥ JavaScript-rendered content
- ’â’´ ’°’ª’°’Ø÷Å’∏÷Ç’¥ session management
- ‘ø’°’≠’æ’°’Æ ’ß content-type headers-’´÷Å

## ‘∂’°÷Ä’£’°÷Å’∏÷Ç’¥

### Tests ’£’∏÷Ä’Æ’°÷Ä’Ø’∏÷Ç’¥
```bash
python -m pytest tests/
```

### Code formatting
```bash
black crawler.py
```

## ‘º’´÷Å’•’∂’¶’´’°

MIT License - ’ø’•’Ω LICENSE ÷Ü’°’µ’¨’® ’¥’°’∂÷Ä’°’¥’°’Ω’∂’•÷Ä’´ ’∞’°’¥’°÷Ä:

## ’Ü’•÷Ä’§÷Ä’∏÷Ç’¥

Pull requests-’® ’∏’≤’ª’∏÷Ç’∂’æ’∏÷Ç’¥ ’•’∂! ‘Ω’∂’§’´÷Ä’∂’•÷Ä’´ ’Ø’°’¥ ’¢’°÷Ä’•’¨’°’æ’∏÷Ç’¥’∂’•÷Ä’´ ’∞’°’¥’°÷Ä:

1. Fork repository
2. ’ç’ø’•’≤’Æ’•÷Ñ feature branch (`git checkout -b feature/amazing-feature`)
3. Commit ÷É’∏÷É’∏’≠’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® (`git commit -m 'Add amazing feature'`)
4. Push branch-’® (`git push origin feature/amazing-feature`)
5. ‘≤’°÷Å’•÷Ñ Pull Request

## ’Ä’•’≤’´’∂’°’Ø

’Å’•÷Ä ’°’∂’∏÷Ç’∂’® - your.email@example.com

GitHub: [@yourusername](https://github.com/yourusername)
