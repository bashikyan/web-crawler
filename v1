#!/usr/bin/env python3
"""
Web Crawler - A simple and efficient web crawler implementation
Author: Your Name
License: MIT
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import json
import csv
from collections import deque
import logging
from datetime import datetime
import os
import argparse
import re


class WebCrawler:
    def __init__(self, max_pages=100, delay=1, timeout=10):
        """
        Initialize the web crawler
        
        Args:
            max_pages (int): Maximum number of pages to crawl
            delay (float): Delay between requests in seconds
            timeout (int): Request timeout in seconds
        """
        self.max_pages = max_pages
        self.delay = delay
        self.timeout = timeout
        self.visited_urls = set()
        self.crawled_data = []
        self.url_queue = deque()
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('crawler.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Headers to mimic a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def is_valid_url(self, url):
        """Check if URL is valid and crawlable"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except:
            return False

    def get_domain(self, url):
        """Extract domain from URL"""
        return urlparse(url).netloc

    def normalize_url(self, url):
        """Normalize URL by removing fragments and query parameters"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def extract_links(self, soup, base_url):
        """Extract all links from the page"""
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            normalized_url = self.normalize_url(full_url)
            
            if self.is_valid_url(normalized_url):
                links.append(normalized_url)
        
        return links

    def extract_page_data(self, soup, url):
        """Extract useful data from the page"""
        try:
            # Extract title
            title = soup.find('title')
            title = title.get_text().strip() if title else "No title"
            
            # Extract meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '') if meta_desc else ''
            
            # Extract headings
            headings = []
            for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                headings.append({
                    'tag': h.name,
                    'text': h.get_text().strip()
                })
            
            # Extract text content (first 500 chars)
            text_content = soup.get_text()
            text_content = re.sub(r'\s+', ' ', text_content).strip()[:500]
            
            # Extract images
            images = []
            for img in soup.find_all('img', src=True):
                img_url = urljoin(url, img['src'])
                images.append({
                    'url': img_url,
                    'alt': img.get('alt', '')
                })
            
            return {
                'url': url,
                'title': title,
                'description': description,
                'headings': headings,
                'text_preview': text_content,
                'images': images[:10],  # Limit to first 10 images
                'crawl_timestamp': datetime.now().isoformat(),
                'links_count': len(self.extract_links(soup, url))
            }
            
        except Exception as e:
            self.logger.error(f"Error extracting data from {url}: {str(e)}")
            return None

    def crawl_page(self, url):
        """Crawl a single page"""
        try:
            self.logger.info(f"Crawling: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '')
            if 'text/html' not in content_type:
                self.logger.warning(f"Skipping non-HTML content: {url}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract page data
            page_data = self.extract_page_data(soup, url)
            if page_data:
                self.crawled_data.append(page_data)
            
            # Extract links for further crawling
            links = self.extract_links(soup, url)
            
            return links
            
        except requests.RequestException as e:
            self.logger.error(f"Request error for {url}: {str(e)}")
            return []
        except Exception as e:
            self.logger.error(f"Unexpected error crawling {url}: {str(e)}")
            return []

    def crawl(self, start_url, same_domain_only=True):
        """
        Start crawling from the given URL
        
        Args:
            start_url (str): Starting URL
            same_domain_only (bool): Whether to crawl only same domain
        """
        if not self.is_valid_url(start_url):
            raise ValueError("Invalid starting URL")
        
        start_domain = self.get_domain(start_url)
        self.url_queue.append(start_url)
        
        self.logger.info(f"Starting crawl from: {start_url}")
        self.logger.info(f"Max pages: {self.max_pages}, Same domain only: {same_domain_only}")
        
        while self.url_queue and len(self.visited_urls) < self.max_pages:
            current_url = self.url_queue.popleft()
            
            # Skip if already visited
            if current_url in self.visited_urls:
                continue
            
            # Skip if different domain (when same_domain_only is True)
            if same_domain_only and self.get_domain(current_url) != start_domain:
                continue
            
            self.visited_urls.add(current_url)
            
            # Crawl the page
            found_links = self.crawl_page(current_url)
            
            # Add new links to queue
            for link in found_links:
                if link not in self.visited_urls:
                    self.url_queue.append(link)
            
            # Respect rate limiting
            time.sleep(self.delay)
            
            # Progress update
            if len(self.visited_urls) % 10 == 0:
                self.logger.info(f"Progress: {len(self.visited_urls)} pages crawled")
        
        self.logger.info(f"Crawling completed. Total pages: {len(self.crawled_data)}")

    def save_to_json(self, filename="crawl_results.json"):
        """Save crawled data to JSON file"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.crawled_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"Data saved to {filename}")
        except Exception as e:
            self.logger.error(f"Error saving to JSON: {str(e)}")

    def save_to_csv(self, filename="crawl_results.csv"):
        """Save crawled data to CSV file"""
        try:
            if not self.crawled_data:
                self.logger.warning("No data to save")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'url', 'title', 'description', 'text_preview', 
                    'links_count', 'crawl_timestamp'
                ])
                writer.writeheader()
                
                for data in self.crawled_data:
                    # Simplify data for CSV
                    csv_row = {
                        'url': data['url'],
                        'title': data['title'],
                        'description': data['description'],
                        'text_preview': data['text_preview'],
                        'links_count': data['links_count'],
                        'crawl_timestamp': data['crawl_timestamp']
                    }
                    writer.writerow(csv_row)
            
            self.logger.info(f"Data saved to {filename}")
        except Exception as e:
            self.logger.error(f"Error saving to CSV: {str(e)}")

    def get_statistics(self):
        """Get crawling statistics"""
        if not self.crawled_data:
            return {}
        
        total_pages = len(self.crawled_data)
        domains = {}
        
        for data in self.crawled_data:
            domain = self.get_domain(data['url'])
            domains[domain] = domains.get(domain, 0) + 1
        
        return {
            'total_pages_crawled': total_pages,
            'unique_domains': len(domains),
            'domain_distribution': domains,
            'average_links_per_page': sum(d['links_count'] for d in self.crawled_data) / total_pages
        }


def main():
    """Main function for command-line usage"""
    parser = argparse.ArgumentParser(description='Web Crawler')
    parser.add_argument('url', help='Starting URL to crawl')
    parser.add_argument('--max-pages', type=int, default=50, help='Maximum pages to crawl')
    parser.add_argument('--delay', type=float, default=1, help='Delay between requests (seconds)')
    parser.add_argument('--timeout', type=int, default=10, help='Request timeout (seconds)')
    parser.add_argument('--output', default='crawl_results', help='Output filename (without extension)')
    parser.add_argument('--format', choices=['json', 'csv', 'both'], default='both', help='Output format')
    parser.add_argument('--cross-domain', action='store_true', help='Allow crawling across different domains')
    
    args = parser.parse_args()
    
    # Create crawler instance
    crawler = WebCrawler(
        max_pages=args.max_pages,
        delay=args.delay,
        timeout=args.timeout
    )
    
    try:
        # Start crawling
        crawler.crawl(args.url, same_domain_only=not args.cross_domain)
        
        # Save results
        if args.format in ['json', 'both']:
            crawler.save_to_json(f"{args.output}.json")
        
        if args.format in ['csv', 'both']:
            crawler.save_to_csv(f"{args.output}.csv")
        
        # Print statistics
        stats = crawler.get_statistics()
        print("\n=== Crawling Statistics ===")
        for key, value in stats.items():
            print(f"{key}: {value}")
        
    except KeyboardInterrupt:
        print("\nCrawling interrupted by user")
    except Exception as e:
        print(f"Error: {str(e)}")


if __name__ == "__main__":
    main()
